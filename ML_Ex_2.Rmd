---
title: "Machine Learning Examples"
subtitle: "Artificial data set"
author: "Stan Mlekodaj"
date: "February 29, 2016"
output: html_document
---
```{r overhead, echo=FALSE, message=FALSE}
# General environment setup
# Clear variables
rm(list=ls())
set.seed(18)
# all par settings which could be changed.
old.par <- par(no.readonly = TRUE)
# Load required packages
library(knitr)
library(xtable)
#library(data.table)
#library(nnet)
library(neuralnet)
#library(randomForest)
library(RSNNS)
library(ROCR)
```
```{r, ref.label="functions", echo=FALSE}
#runs chunk with all function definitions
```

## R Packages for Machine Learning
- nnet
- neuralnet
- randomForest
- RSNNS
- deepnet
- H20   (several models including deep learning)
- caret (high level, interfaces with other models)
- rpart (decision trees) Also partykit package for visualization.

## To-Do's
- Training outputs skewness check
- Feature selection/elimination 
    - some models do automatically. i.e. caret
- Feature normalization (caret package?)
- Gradient Checking (usually not possible with off the shelf packages)
- Selecting model complexity (polynomial degree) vs. cross-validation error (caret)
- Selecting lambda (regularization parameter) vs. CV error (caret grid)
- Plotting Learning Curve to detect high bias or variance (mlp in RSNNS)
- Error analysis in CV errors. Trying to categorize CV error sets.
- Determine threshold for output classification
    * Use performance() function to plot measures vs. cutoff.
    * Currently, ROCR supports only binary classification.
    * caret also has tuneGrid input in train()
- F Score, Accuracy, Precision, Recall

## Artificial data set for model testing
Idea:  Create input/output data set from a set of equations & random variables. 
This will create a known model with known correlations & variable importance.

```{r echo=FALSE}
# linear output data set
samples <- 1000

# linear output
# feat1 = rnorm(samples)
# feat2 = rnorm(samples)
# feat3 = 0.5 * (feat1 + rnorm(samples))
# feat4 = 0.9 * feat2
# feat5 = rnorm(samples)
# features <- data.frame(feat1, feat2, feat3, feat4, feat5)
# features <- scale(features,
#                   center = c(-5, 3, 50, 1000, -200), 
#                   scale = c(1/5, 1/3, 1/50, 1/1000, 1/200)
#                   )
# output = 7 + 0.1*feat1 + 0.2*feat2 + 0.3*feat3
# arbData <- data.frame(features, output)

# logistic output
output <- round(runif(samples))
feat1 <- 0.10 * output + rnorm(samples)
feat2 <- 0.25 * output + rnorm(samples)
feat3 <- 0.50 * output + rnorm(samples)
#eat3 <- 0.50 * (feat1 + rnorm(samples))
#feat4 <- 0.90 * feat2
feat4 <- 0.90 * output + rnorm(samples)
feat5 <- rnorm(samples)
arbData <- data.frame(feat1, feat2, feat3, feat4, feat5, output)

head(arbData)

```

## H2O Package
Installation requires Rcurl, jsonlite & statmod packages required.  Need to specify
repos if R gives error such as this:

"Package which is only available in source form, and may need compilation of C/C++/Fortran: ‘statmod'"

        install.packages("RCurl", repos="http://cran.us.r-project.org")
        install.packages("jsonlite", repos="http://cran.us.r-project.org")
        install.packages("statmod", repos="http://cran.us.r-project.org")

install.packages("h2o")

or (preferred), download latest version from [link](http://www.h2o.ai/download/)


## General flow

    1. Define data inputs, ouputs. Make sure outputs are ~50/50 outcomes (not skewed)
        a. Feature selection/elimination, PCA
    2. normalize data (except ouputs when binary)
        a. dimensionality reduction/PCA (optional) 
    3. Split data into training, cross-validation, test sets
    4. Train model on training set
    5. Tune model parameters for best cross-validation performance
    6. Evaluate model on test set
    
## Model Evalulation
ROCR package prediction() & performance(). Can plot metrics vs. cutoff to find best threshold
for binary classifier output.

Need to determine what predictor metric is important.

    1. run model on trainInputs & trainOutputs
    2. predict on cvInputs
    3. make predict object using prediction(predict, cvOutputs), 
    4. feed it into performance()
    5. choose best "cutoff"
    6. report performance metrics for that cutoff
    7. predict on testInputs, report performance
    
## Lessons learned
- neural networks
    - SOme literature suggests tanh over logistic function produces better results because 
    it is symmetric around 0.
    - neuralnet() does not take "y ~ ." type of formula. Must write out long formula instead.
    
    
## Data Definitions

```{r,echo=FALSE}
♠Y# ############ define data, model formula (training sets in rows) #######
# rawData <- arbData
# 
# # model formula definition (neuralnet, nnet)
# #fo <- formula("output ~ .")
# # this approach ensures compatibility with neuralnet() which does not take "y ~ ."
# n <- names(rawData)
# fo <- as.formula(paste("output ~", paste(names(rawData)[!n %in% "output"], collapse = " + ")))
# 
# # input & output column indexes
# inCols <- c(1:5)
# outCols <- c(6)
# 
# # Which columns to normalize - 0/1 outputs/inputs dont need scaling.
# normCols <- c(1:5)
# #normCols <- c()
# #######################################################################

############ define data, model formula (training sets in rows) #######
rawData <- infert

# model formula definition (neuralnet, nnet)
fo <- formula("case ~ age + parity + induced + spontaneous")

# input & output column indexes
inCols <- c(2, 3, 4, 6)
outCols <- c(5)

# Which columns to normalize - 0/1 outputs/inputs dont need scaling.
normCols <- c(2, 3, 4, 6, 7, 8)
#######################################################################

nInputs <- nrow(rawData)
featureNames <- colnames(rawData)

# Feature Normalization
allData <- rawData
allData[ , normCols] <- normalizeData(rawData[ , normCols], type = "norm") #also type = "0_1" or "center"

# randomize rows
allData <- allData[sample(nInputs), ]

# split data in training, cross-validation, and test sets
trainBreak <- nInputs * 0.6 # 60% train
cvBreak <- nInputs * 0.8    # 20% cv, 20% test
trainData <- allData[1 : trainBreak, ]
cvData <- allData[(trainBreak + 1) : cvBreak, ]
testData <- allData[(cvBreak + 1) : nInputs, ]

# for RSNNS - split into input & output matrixes 
trainInputs <- trainData[, inCols]
trainOutputs <- trainData[, outCols]
cvInputs <- cvData[, inCols]
cvOutputs <- cvData[, outCols]
testInputs <- testData[, inCols]
testOutputs <- testData[, outCols]
```

Input Data Information:

- Number of input features:  `r length(inCols)`
- Number of outputs:         `r length(outCols)`
- Number of total data sets: `r nInputs`


|            |Training Data            | Cross-Validation Data | Test Data              |
|------------|-------------------------|-----------------------|------------------------|
|Skewness    | `r table(trainOutputs)` | `r table(cvOutputs) ` | `r table(testOutputs)` |
|Data Sets   | `r nrow(trainData)`     | `r nrow(cvData) `     | `r nrow(testData)`     |


## Linear Regression

```{r,echo=FALSE}
model_lm <- lm(fo, data = trainData)
model_lm

predict_cv_lm <- predict(model_lm, cvInputs)
predict_test_lm <- predict(model_lm, testInputs)

# Evaluate performance on cross-validation set
cvCutoff_lm <- getCutoffAndPlot(predict_cv_lm, cvOutputs)
cutoff_cv_lm <- cvCutoff_lm[[1]]
perfMetricName_cv_lm <- cvCutoff_lm[[2]]
perfMetric_cv_lm <- cvCutoff_lm[[3]]

#Repeat on test set
testCutoff_lm <- getCutoffAndPlot(predict_test_lm, testOutputs)
cutoff_test_lm <- testCutoff_lm[[1]]
perfMetricName_test_lm <- testCutoff_lm[[2]]
perfMetric_test_lm <- testCutoff_lm[[3]]

```

Cross-Validation set `r perfMetricName_cv_lm` = `r round(perfMetric_cv_lm, 3)`

Test set `r perfMetricName_test_lm` = `r round(perfMetric_test_lm, 3)`

# Generalized Linear Model

```{r,echo=FALSE}
model_glm <- glm(fo, data = trainData, family = gaussian())
model_glm <- glm(fo, data = trainData, family = binomial())
model_glm

# caret: same as using glm()
# library(caret)
# glmTune <- train(fo, data = trainData, method = "glm",  family = binomial())
# glmTune

predict_cv_glm <- predict(model_glm, cvInputs)
predict_test_glm <- predict(model_glm, testInputs)

# Evaluate performance on cross-validation set
cvCutoff_glm <- getCutoffAndPlot(predict_cv_glm, cvOutputs)
cutoff_cv_glm <- cvCutoff_glm[[1]]
perfMetricName_cv_glm <- cvCutoff_glm[[2]]
perfMetric_cv_glm <- cvCutoff_glm[[3]]

#Repeat on test set
testCutoff_glm <- getCutoffAndPlot(predict_test_glm, testOutputs)
cutoff_test_glm <- testCutoff_glm[[1]]
perfMetricName_test_glm <- testCutoff_glm[[2]]
perfMetric_test_glm <- testCutoff_glm[[3]]
```

Cross-Validation set `r perfMetricName_cv_glm` = `r round(perfMetric_cv_glm, 3)`

Test set `r perfMetricName_test_glm` = `r round(perfMetric_test_glm, 3)`

## RSNNS Neural Network - MLP 

```{r,echo=FALSE}
# neural network using RSNNS 
# model <- mlp(trainInputs, trainOutputs,
#              size=2,
#              learnFunc="Quickprop", learnFuncParams=c(0.05, 2.0, 0.00005, 0),
#              maxit=500,
#              linOut = FALSE,
#              inputsTest = cvInputs,
#              targetsTest = cvOutputs
#              )
model <- mlp(x = trainInputs, 
             #y = as.numeric(trainOutputs),
             y = trainOutputs,
                size=c(5,3),
                learnFunc="BackpropWeightDecay", 
                learnFuncParams=c(0.05, .0000005, 0, 0),
                maxit=500,
                inputsTest = testInputs,
                targetsTest = testOutputs,
                linOut = FALSE
)
#print(model)
summary(model)
#weightMatrix(model)
#extractNetInfo(model)

# Prediction on cross-validation data
predict_cv <- predict(model, cvInputs)
# predict on test set
predict_test <- predict(model, testInputs)

# Initial model plots on cross-validation data
par(mfrow=c(2,2))
plotIterativeError(model) # SSE error vs. training iteration
plotRegressionError(cvOutputs, predict_cv) # regression quality
plotROC(fitted.values(model), trainOutputs) # ROC on training data
plotROC(predict_cv, cvOutputs) # cross validation ROC

# Evaluate performance on cross-validation set
cvCutoff <- getCutoffAndPlot(predict_cv, cvOutputs)
cutoff_cv <- cvCutoff[[1]]
perfMetricName_cv <- cvCutoff[[2]]
perfMetric_cv <- cvCutoff[[3]]

# # make predicitons object using ROCR - ONLY FOR BINARY CLASSIFICATION
# pred_cv_obj <- ROCR::prediction(predict_cv, cvOutputs) # nerualnet also has prediction()
# # performance - need decision on most important metric
# perf_cv_obj <- ROCR::performance(pred_cv_obj, "acc", "cutoff")
# maxMetricInd <- which.max(perf_cv_obj@y.values[[1]])    # index of metric max
# cutoff_cv <- pred_cv_obj@cutoffs[[1]][maxMetricInd]     # cutoff for max metric 
# perfMetric_cv <- perf_cv_obj@y.values[[1]][maxMetricInd]   # metric value at cutoff
# perfMetricName <- perf_cv_obj@y.name                    # metric name
# 
# # Plot several metrics vs. cutoff
# par(mfrow=c(2,2))
# plot(ROCR::performance(pred_cv_obj, "acc", "cutoff"), main = "Accuracy")
# plot(ROCR::performance(pred_cv_obj, "f", "cutoff"), main = "F-Score")
# plot(ROCR::performance(pred_cv_obj, "phi", "cutoff"), main = "Phi Corr. Coef.")
# plot(ROCR::performance(pred_cv_obj, "sar", "cutoff"), main = "SAR Multi-Score")

#Repeat on test set
testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]

# # make predicitons object using ROCR - ONLY FOR BINARY CLASSIFICATION
# pred_test_obj <- ROCR::prediction(predict_test, testOutputs) # nerualnet also has prediction()
# # performance - need decision on most important metric
# perf_test_obj <- ROCR::performance(pred_test_obj, "acc", "cutoff")
# maxMetricInd <- which.max(perf_test_obj@y.values[[1]])    # index of metric max
# cutoff_test <- pred_test_obj@cutoffs[[1]][maxMetricInd]     # cutoff for max metric 
# perfMetric_test <- perf_test_obj@y.values[[1]][maxMetricInd]   # metric value at cutoff
# perfMetricName <- perf_test_obj@y.name                    # metric name
# 
# # Plot several metrics vs. cutoff
# par(mfrow=c(2,2))
# plot(ROCR::performance(pred_test_obj, "acc", "cutoff"), main = "Accuracy")
# plot(ROCR::performance(pred_test_obj, "f", "cutoff"), main = "F-Score")
# plot(ROCR::performance(pred_test_obj, "phi", "cutoff"), main = "Phi Corr. Coef.")
# plot(ROCR::performance(pred_test_obj, "sar", "cutoff"), main = "SAR Multi-Score")



# 
# confusionMatrix(trainOutputs, round(fitted.values(model)))
# confusionMatrix(cvOutputs, round(predictions))
# confusionMatrix(testOutputs , round(predict(model, testInputs)))


```

Cross-Validation set `r perfMetricName_cv` = `r round(perfMetric_cv, 3)`

Test set `r perfMetricName_test` = `r round(perfMetric_test, 3)`


## neuralnet

```{r,echo=FALSE}
# neuralnet package
nn <- neuralnet(fo, data = trainData,
                hidden=5,
                threshold = 0.01,
                stepmax = 1e+5,
                learningrate = 0.01,
                lifesign = "full",
                algorithm = "rprop+",
                err.fct = "ce",
                act.fct = "logistic",
                linear.output = FALSE
                )

nn
summary(nn)
plot(nn)
# Cross-Validation & Test set Predictions
nn_predict_cv <- compute(nn, covariate = cvInputs)
nn_predict_test <- compute(nn, covariate = testInputs)

# performance & cutoff selection
# Evaluate performance on cross-validation set
cvCutoff_nn <- getCutoffAndPlot(nn_predict_cv$net.result, cvOutputs)
cutoff_cv_nn <- cvCutoff_nn[[1]]
perfMetricName_cv_nn <- cvCutoff_nn[[2]]
perfMetric_cv_nn <- cvCutoff_nn[[3]]

#Repeat on test set
testCutoff_nn <- getCutoffAndPlot(nn_predict_test$net.result, testOutputs)
cutoff_test_nn <- testCutoff_nn[[1]]
perfMetricName_test_nn <- testCutoff_nn[[2]]
perfMetric_test_nn <- testCutoff_nn[[3]]




```

Cross-Validation set `r perfMetricName_cv_nn` = `r round(perfMetric_cv_nn, 3)`

Test set `r perfMetricName_test_nn` = `r round(perfMetric_test_nn, 3)`

##  Example from RSNNS R package documentation
```{r,echo=FALSE}
####  Example from RSNNS R package documentation
library(RSNNS)
rm(list=ls())
set.seed(18)
data(iris)
#shuffle the vector
iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]

irisValues <- iris[,1:4]
irisTargets <- decodeClassLabels(iris[,5])
#irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)
iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)
iris <- normTrainingAndTestSet(iris)
model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),
maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)
summary(model)
model
weightMatrix(model)
extractNetInfo(model)
par(mfrow=c(2,2))
plotIterativeError(model)
predictions <- predict(model,iris$inputsTest)
plotRegressionError(predictions[,2], iris$targetsTest[,2])
confusionMatrix(iris$targetsTrain,fitted.values(model))
confusionMatrix(iris$targetsTest,predictions)
plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])
plotROC(predictions[,2], iris$targetsTest[,2])
#confusion matrix with 402040-method
confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),
method="402040", l=0.4, h=0.6))
```


```{r,echo=FALSE,fig.width=8,fig.height=6,fig.align='left',dpi=72,fig.retina=3}
```

```{r, echo=FALSE, eval=FALSE}
# example of using performance() function in ROCR
# Currently, ROCR supports only binary classification
# first need to create a prediction object. 
# By default, x.measure='cutoff' input in performance(), i.e. always get measure vs. cutoff
# Can use to find best cutoff.
library(ROCR)
test_pred <- c(rep(0.8, 10), rep(0.4, 10))
test_pred <- rnorm(20)
test_true <- c(rep(1, 8), rep(0, 12))

pred_obj <- ROCR::prediction( test_pred, test_true)
pred_obj
perf_obj <- performance(pred_obj, "sar", "cutoff")
perf_obj <- performance(pred_obj, "f", "cutoff")
perf_obj <- performance(pred_obj,"tpr","fpr")
plot(perf_obj)
```



```{r functions, echo=FALSE, eval=FALSE}
# Define functions in this chunk

##############  NEEDS VERIFICATION
# Takes predicted & target values, computes best prediction cutoff for max accuracy.
# ONLY FOR BINARY CLASSIFICATION - targets values must have only 2 values
# Returns cutoff and resulting accuracy. Also plots 4 metrics vs. cutoffs
getCutoffAndPlot <- function(predicted, targets) {
   
#     par(mfrow=c(1,1))
#     plotROC(predicted, targets, main = "ROC Curve") # ROC curve
    
    # make predicitons object using ROCR
    pred_obj <- ROCR::prediction(predicted, targets) # nerualnet also has prediction()
    # performance - need decision on most important metric
    perf_obj <- ROCR::performance(pred_obj, "acc", "cutoff")
    maxMetricInd <- which.max(perf_obj@y.values[[1]])    # index of metric max
    cutoff <- pred_obj@cutoffs[[1]][maxMetricInd]     # cutoff for max metric 
    perfMetric <- perf_obj@y.values[[1]][maxMetricInd]   # metric value at cutoff
    perfMetricName <- perf_obj@y.name                    # metric name
    
    # Plot several metrics vs. cutoff
    par(mfrow=c(2,2))
    plot(ROCR::performance(pred_obj, "acc", "cutoff"), ylim = c(0,1), main = "Accuracy")
    plot(ROCR::performance(pred_obj, "f", "cutoff"), ylim = c(0,1), main = "F-Score")
    plot(ROCR::performance(pred_obj, "phi", "cutoff"), ylim = c(-1,1), main = "Phi Corr. Coef.")
    plot(ROCR::performance(pred_obj, "sar", "cutoff"), main = "SAR Multi-Score")
    
    return(list(cutoff, perfMetricName, perfMetric))
}


```

```{r opt_neuralnet, echo=FALSE, eval=FALSE}
# Define functions in this chunk
# opt_neuralnet <- function(param = "hidden", values = list(c(4), c(7), c(10)),
#                           ) {
#     nn <- neuralnet(fo, data = trainData,
#                     hidden=2,
#                     threshold = 0.01,
#                     stepmax = 1e+5,
#                     learningrate = 0.01,
#                     lifesign = "full",
#                     algorithm = "rprop+",
#                     err.fct = "ce",
#                     act.fct = "logistic",
#                     linear.output = FALSE
#                     )
# }

lr <- c(0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5)
th <- c(0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5)

for (i in 1:length(lr)) {
    lr_var <- lr[i]
    for (j in 1:length(th)) {
        th_var <- th[j]
        nn_int <- neuralnet(fo, data = trainData,
                        hidden=5,
                        threshold = th_var,
                        stepmax = 1e+5,
                        learningrate = lr_var,
                        lifesign = "full",
                        algorithm = "rprop+",
                        err.fct = "ce",
                        act.fct = "logistic",
                        linear.output = FALSE
                        )
        pred_int <- compute(nn_int, covariate = cvInputs)
        
        ???
    }
}


```
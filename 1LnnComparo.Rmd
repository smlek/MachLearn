---
title: "1-Layer 1-Neuron Neural Network Comparizon on Artificial Data"
author: "Stan Mlekodaj"
date: "March 8, 2016"
output: html_document
---

```{r overhead, echo=FALSE, message=FALSE}
# General environment setup
# Clear variables
rm(list=ls())
# all par settings which could be changed.
old.par <- par(no.readonly = TRUE)
# Load required packages
library(knitr)
#library(xtable)
#library(data.table)
#library(nnet)
#library(neuralnet)
#library(randomForest)
library(RSNNS)
#library(ROCR)
#library(caret)
```
```{r, ref.label="functions", echo=FALSE}
#runs chunk with all function definitions
```

## To-Do's


- BackpropThroughTime


## Lessons learned
- mlp() function (and probably others in RSNNS) want matrix input, not DF
- caret mlpWeightDecayML and mlpWeightDecay dont work if using "decay" in grid tune
  and also specifying learnFuncParams.  internal call places "decay" at 
  learnFuncParams[3] not [2]
  

## Artificial data set for model testing
Idea:  Create input/output data set from a set of equations & random variables. 
This will create a known model with known correlations & variable importance.

```{r echo=FALSE}
# linear output data set
samples <- 1000

# logistic output
output <- 3*round(runif(samples))
#output <- 5*runif(samples)

feat1 <- 0.8 * output + rnorm(samples)
feat2 <- -0.5 * output + rnorm(samples)
feat3 <- feat2^2
feat4 <- rnorm(samples)
#output <- factor(output, levels=c("0","1"), labels=c("Down","Up"))
arbData <- data.frame(feat1, feat2, feat3, feat4, output)

head(arbData)

matplot(arbData[,-5],arbData[,5])
par(mfrow=c(2,2))
for (i in 1:(ncol(arbData) - 1)) {
    plot(arbData[,i],output, main = colnames(arbData)[i] )
}
round(cor(arbData),2)

```

    
## Data Definitions

```{r,echo=FALSE}
############ define data, model formula (training sets in rows) #######
rawData <- arbData

# model formula definition (neuralnet, nnet)
# this approach ensures compatibility with neuralnet() which does not take "y ~ ."
n <- names(rawData)
allOutColNames <- "output"
outputColumn <- "output"
inColNames <- n[!(n %in% allOutColNames)]
fo <- as.formula(paste(outputColumn, "~", paste(inColNames, collapse = " + ")))

# input & output column indexes
inCols <- which(n %in% inColNames)
outCols <- which(n %in% outputColumn)

linearOutput <- FALSE        # linear output=TRUE, factor output=FALSE
normalizeOutputs <- TRUE
inputNormType <- "norm"     # type = "norm", "0_1" or "center"
outputNormType <- "0_1" 
mlpHiddenActFunc <- "Act_Logistic" # Act_Logistic, Act_TanH
deCorrelateInputs <- FALSE
#######################################################################

allData <- rawData
data_size <- nrow(allData)
featureNames <- colnames(allData)

# shuffle & split data in training and test sets 
# Cross-Validation set omitted since using repeatedcv in caret
allData <- allData[sample(data_size), ]
splitIndex <- data_size * 0.8
trainData <- allData[1 : splitIndex, ]
testData <- allData[(splitIndex + 1) : data_size, ]

# # Feature Normalization
# allData[ , normCols] <- normalizeData(rawData[ , normCols], type = "norm") #also type = "0_1" or "center"
# # randomize rows
# allData <- allData[sample(data_size), ]
# 
# # split data in training, cross-validation, and test sets
# trainBreak <- data_size * 0.6 # 60% train
# cvBreak <- data_size * 0.8    # 20% cv, 20% test
# trainData <- allData[1 : trainBreak, ]
# cvData <- allData[(trainBreak + 1) : cvBreak, ]
# testData <- allData[(cvBreak + 1) : data_size, ]

# split into input & output matrixes 
#trainInputs <- as.matrix(trainData[, inCols])
trainInputs <- trainData[, inCols]
trainOutputs <- trainData[, outCols]
#testInputs <- as.matrix(testData[, inCols])
testInputs <- testData[, inCols]
testOutputs <- testData[, outCols]

# # Input Normalization (caret) - center/scale
# preProcValues <- preProcess(trainInputs, method = c("center", "scale"))
# trainInputs <- predict(preProcValues, trainInputs)
# # test inputs normalized using training data normalization values
# testInputs <- predict(preProcValues, testInputs)

# # Input Normalization (RSNNS) - center/scale
trainInputs <- normalizeData(trainInputs, type = inputNormType) #also type = "0_1" or "center"
# # test inputs normalized using training data normalization values
testInputs <- normalizeData(testInputs, type = attr(trainInputs, "normParams"))

# normalize outputs
if(normalizeOutputs) {
    trainOutputs <- normalizeData(trainOutputs, type = outputNormType) #also type = "0_1" or "center"
    testOutputs <- normalizeData(testOutputs, type = attr(trainOutputs, "normParams")) 
}

# Remove correlated inputs
if(deCorrelateInputs) {
    corCols <- findCorrelation(cor(trainInputs), cutoff = 0.9, names = FALSE )
    if(length(corCols) > 0) {
            trainInputs <- trainInputs[, -corCols]
            testInputs <- testInputs[, -corCols]  
    }
    
}
# # Input Normalization - PCA
# preProcValuesPCA <- preProcess(trainInputs, method = "pca")
# trainInputs <- predict(preProcValuesPCA, trainInputs)
# # test inputs normalized using training data normalization values
# testInputs <- predict(preProcValuesPCA, testInputs)

# reconstruct train & test dataframes
trainData <- data.frame(cbind(trainInputs, trainOutputs))
colnames(trainData) <- featureNames
testData <- data.frame(cbind(testInputs, testOutputs))
colnames(testData) <- featureNames
```

Raw Input Data Information:

- Number of input features:  `r length(inCols)`
- Number of outputs:         `r length(outCols)`
- Number of total data sets: `r data_size`

|            |Training Data            | Test Data              |
|------------|-------------------------|------------------------|
|Skewness    | `r table(trainOutputs)` | `r table(testOutputs)` |
|Data Sets   | `r nrow(trainData)`     | `r nrow(testData)`     |

Prediction Formula:  `r print(fo)`

## MLP Weight Decay

```{r,echo=FALSE}
#library(RSNNS)
model <- mlp(x = trainInputs, 
                #y = as.numeric(trainOutputs),
                y = trainOutputs,
                size=c(1),
                maxit=500,
                #size = C(4,2)
                learnFunc="BackpropWeightDecay", 
                learnFuncParams=c(0.01, .0005, 0, 0),
                hiddenActFunc = mlpHiddenActFunc,
                #hiddenActFunc = "Act_Logistic",
                inputsTest = testInputs,
                targetsTest = testOutputs,
                linOut = linearOutput
)
#print(model)
summary(model)
# predict on test set
predict_test <- predict(model, testInputs)

MLPclassificationPlots(model=model, trainOutputs=trainOutputs, predicted=predict_test, targets=testOutputs)
    
testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]


```

## MLP RPROP
```{r,echo=FALSE}
#library(RSNNS)
model <- mlp(x = trainInputs, 
                #y = as.numeric(trainOutputs),
                y = trainOutputs,
                size=1,
                maxit=500,
                #size = C(4,2)
                learnFunc="Rprop", 
                learnFuncParams=c(0.01, 50, 6),
                hiddenActFunc = mlpHiddenActFunc,
                inputsTest = testInputs,
                targetsTest = testOutputs,
                linOut = linearOutput
)
#print(model)
summary(model)
# predict on test set
predict_test <- predict(model, testInputs)

MLPclassificationPlots(model=model, trainOutputs=trainOutputs, predicted=predict_test, targets=testOutputs)
    
testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]

```

## MLP BackpropMomentun
```{r,echo=FALSE}
#library(RSNNS)
model <- mlp(x = trainInputs, 
                #y = as.numeric(trainOutputs),
                y = trainOutputs,
                size=1,
                maxit=500,
                #size = C(4,2)
                learnFunc="BackpropMomentum", 
                learnFuncParams=c(0.01, 0.5, 0.1, 0),
                hiddenActFunc = mlpHiddenActFunc,
                inputsTest = testInputs,
                targetsTest = testOutputs,
                linOut = linearOutput
)
#print(model)
summary(model)
# predict on test set
predict_test <- predict(model, testInputs)

MLPclassificationPlots(model=model, trainOutputs=trainOutputs, predicted=predict_test, targets=testOutputs)
    
testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]

```

## MLP RpropMAP
```{r,echo=FALSE}
#library(RSNNS)
model <- mlp(x = trainInputs, 
                #y = as.numeric(trainOutputs),
                y = trainOutputs,
                size=1,
                maxit=500,
                #size = C(4,2)
                learnFunc="RpropMAP", 
                learnFuncParams=c(0.01, 0.01, 0.00005, 20, 0),
                hiddenActFunc = mlpHiddenActFunc,
                inputsTest = testInputs,
                targetsTest = testOutputs,
                linOut = linearOutput
)
#print(model)
summary(model)
# predict on test set
predict_test <- predict(model, testInputs)

MLPclassificationPlots(model=model, trainOutputs=trainOutputs, predicted=predict_test, targets=testOutputs)
    
testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]

```


## Elman
```{r,echo=FALSE}
model <- elman(x = trainInputs,
                    y = trainOutputs,
                    size=1,
                    maxit=500,
                    initFunc = "JE_Weights",
                    initFuncParams = c(1, -1, 0.3, 1, 0.5),
                    learnFunc = "JE_BP",
                    learnFuncParams = c(0.2),
                    updateFunc = "JE_Order",
                    updateFuncParams = c(0),
                    inputsTest = testInputs,
                    targetsTest = testOutputs,
                    linOut = linearOutput
                    )
summary(model)
# predict on test set
predict_test <- predict(model, testInputs)

MLPclassificationPlots(model=model, trainOutputs=trainOutputs, predicted=predict_test, targets=testOutputs)
    
testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]
```

## Jordan
```{r,echo=FALSE}
model <- jordan(x = trainInputs,
                    y = trainOutputs,
                    size=1,
                    maxit=500,
                    initFunc = "JE_Weights",
                    initFuncParams = c(1, -1, 0.3, 1, 0.5),
                    learnFunc = "JE_BP",
                    learnFuncParams = c(0.2),
                    updateFunc = "JE_Order",
                    updateFuncParams = c(0),
                    inputsTest = testInputs,
                    targetsTest = testOutputs,
                    linOut = linearOutput
                    )
summary(model)
# predict on test set
predict_test <- predict(model, testInputs)

MLPclassificationPlots(model=model, trainOutputs=trainOutputs, predicted=predict_test, targets=testOutputs)
    
testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]
```


## DNN

```{r,echo=FALSE, eval=FALSE}
library(deepnet)
library(caret)
# cvCtrl <- trainControl(method = "repeatedcv", repeats = 1,
#                        #summaryFunction = twoClassSummary,
#                        classProbs = TRUE
#                        )
cvCtrl <- trainControl(method = "none", number = 1,
                       #summaryFunction = twoClassSummary,
                       classProbs = TRUE
                       )
#tune_grid <- expand.grid(size = c(1,3,5))
tune_grid <- expand.grid(layer1 = c(1),
                         layer2 = 0,
                         layer3 = 0,
                         hidden_dropout = 0,
                         visible_dropout = 0
                         )

set.seed(1)
dnnModel <- train(#x = trainInputs,
                 x = trainData[, inCols],
                 y = trainOutputs,
                 #y = factor(trainOutputs, levels=c("0","1"), labels=c("Down","Up")),
                 method = "dnn",
                 #tuneLength = 30,
                 metric = "Accuracy",
                 trControl = cvCtrl,
                 tuneGrid = tune_grid
                 )
dnnModel
dnnModelPredict <- predict(dnnModel, testInputs)
caret::confusionMatrix(dnnModelPredict, testOutputs)

```

## H20 Deep Learning
```{r,echo=FALSE}
library(h2o)
h2o.init(ip = "localhost", port = 54321, nthreads = -2, max_mem_size = "3g")
#demo(h2o.deeplearning)



h2oModel <- h2o.deeplearning(x = inColNames, 
                             y = outputColumn, 
                             training_frame = as.h2o(trainData),
                             validation_frame = as.h2o(testData),
                             activation = "Rectifier",
                             #activation = "TanH",
                             hidden = c(1),
                             epochs = 500,
                             #nfolds = 5,
                             l1 = 1e-5,
                             score_interval = 0.1,
                             score_training_samples = 0,
                             export_weights_and_biases = TRUE,
                             variable_importances = TRUE
                            )
h2oModel
h2o.weights(h2oModel)
h2o.biases(h2oModel)
h2o.scoreHistory(h2oModel)
h2o.varimp(h2oModel)
feat.l1 <- h2o.deepfeatures(h2oModel, data = as.h2o(trainData), layer = 1)
# h2o.no_progress()
# h2o.show_progress()
# h2o.shutdown()


# h2oModelPredict <- predict(h2oModel, as.h2o(testData))
# h2operformance = h2o.performance(model = h2oModel, valid = TRUE)
# h2operformance = h2o.performance(model = h2oModel)
#print(h2operformance)


```

## H20 GLM
```{r,echo=FALSE}
h2oModel <- h2o.glm(x = inColNames, 
                    y = outputColumn, 
                    training_frame = as.h2o(trainData),
                    validation_frame = as.h2o(testData),
                    family = "binomial"
                    #nfolds = 5
                    )
h2oModel
# h2operformance = h2o.performance(model = h2oModel, valid = TRUE)
# h2operformance = h2o.performance(model = h2oModel)
# print(h2operformance)

```

## GLM
```{r,echo=FALSE}

# seems to give same coefficients
model_glm <- glm(fo, data = trainData, family = binomial())
#model_glm <- glm(fo, data = trainData, family = gaussian())
#model_glm <- glm(x = trainInputs, y = trainOutputs, family = gaussian())
#model_glm


summary(model_glm)
# predict on test set
predict_test <- predict(model_glm, testData)

# MLPclassificationPlots(model=model_glm, trainOutputs=trainOutputs, predicted=predict_test, targets=testOutputs)

#Initial model plots on cross-validation data
par(mfrow=c(2,2))
plot(testOutputs, predict_test, xlim=range(testOutputs), ylim=range(predict_test)) # regression quality
plotROC(fitted.values(model_glm), trainOutputs) # ROC on training data
plotROC(predict_test, testOutputs)

testCutoff <- getCutoffAndPlot(predict_test, testOutputs)
cutoff_test <- testCutoff[[1]]
perfMetricName_test <- testCutoff[[2]]
perfMetric_test <- testCutoff[[3]]

```












```{r functions, echo=FALSE, eval=FALSE}
# Define functions in this chunk

# Takes predicted & target values, computes best prediction cutoff for max accuracy.
# ONLY FOR BINARY CLASSIFICATION - targets values must have only 2 values
# Returns cutoff and resulting accuracy. Also plots 4 metrics vs. cutoffs
getCutoffAndPlot <- function(predicted, targets) {
   
#     par(mfrow=c(1,1))
#     plotROC(predicted, targets, main = "ROC Curve") # ROC curve
    library(ROCR)
    # make predicitons object using ROCR
    pred_obj <- ROCR::prediction(predicted, targets) # nerualnet also has prediction()
    # performance - need decision on most important metric
    perf_obj <- ROCR::performance(pred_obj, "f", "cutoff")
    maxMetricInd <- which.max(perf_obj@y.values[[1]])    # index of metric max
    cutoff <- pred_obj@cutoffs[[1]][maxMetricInd]     # cutoff for max metric 
    perfMetric <- perf_obj@y.values[[1]][maxMetricInd]   # metric value at cutoff
    perfMetricName <- perf_obj@y.name                    # metric name
    
    # Plot several metrics vs. cutoff
    par(mfrow=c(1,2))
    plot(ROCR::performance(pred_obj, "acc", "cutoff"), ylim = c(0,1), main = "Accuracy")
    plot(ROCR::performance(pred_obj, "f", "cutoff"), ylim = c(0,1), main = "F-Score")
#     plot(ROCR::performance(pred_obj, "phi", "cutoff"), ylim = c(-1,1), main = "Phi Corr. Coef.")
#     plot(ROCR::performance(pred_obj, "sar", "cutoff"), main = "SAR Multi-Score")
    
    return(list(cutoff, perfMetricName, perfMetric))
}

MLPregressionPlots <- function(model, trainOutputs, predicted, targets){
    par(mfrow=c(1,2))
    #errlim <- range(c(range(model$IterativeFitError),range(model$IterativeTestError)))
    plotIterativeError(model) # SSE error vs. training iteration
    plot(targets,,predicted, xlim=range(targets), ylim=range(predicted)) # regression qualiry

}

MLPclassificationPlots <- function(model, trainOutputs, predicted, targets){
    par(mfrow=c(2,2))
    #errlim <- range(c(range(model$IterativeFitError),range(model$IterativeTestError)))
    plotIterativeError(model, 
                       main="Iteration Error - Test Error in Red") # SSE error vs. training iteration
    plot(targets, predicted, 
         xlim = range(targets),
         ylim = range(predicted),
         main = "Regression Quality"
         ) # regression qualiry
    plotROC(fitted.values(model), trainOutputs, main = "ROC - Train Data") # ROC on training data
    plotROC(predicted, targets, main = "ROC - Test Data")
}

    
```


